title: 无废话，微调+数据检索+打包部署 Deepseek 一条龙

categories: []
date: 2024-04-15 21:12:30
---
# 微调

1.  下载模型，Safetensors 格式就行 <https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1>
2.  拉取 LLaMA-Factory ，看文档运行起来 <https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md>
3.  依赖可能没有用到 Cuda 加速，看这个选择你自己的 cuda 版本，安装就好 <https://pytorch.org/get-started/locally/>
4.  修改 LLaMA-Factory/data/identity.json，比如将模型名字和制作者换掉。也可以去找数据集放到这个目录下
5.  看图，蓝色地方先不管，先根据图上修改红色的地方，然后开始训练即可

<!--more-->


![1.png](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/b3cefda129b3454c9b5853229f7bfaa7~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAgREZsYW5kcw==:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzYzMjQ0MjE0Nzg3OTYzOSJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1748270614&x-orig-sign=nn6VBk2NayZRSHgN5ODAM6teGos%3D)

6.  注意资源占用，如果用的 CPU 就是 pytorch 版本不对

# 使用

![2.png](https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/de6ca4a0a3fb4e90af454009d21df607~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAgREZsYW5kcw==:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMzYzMjQ0MjE0Nzg3OTYzOSJ9&rk3s=f64ab15b&x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&x-orig-expires=1748270614&x-orig-sign=ZBS5eWLTbqXsUOvl9QWBP2tgfl4%3D)

1.  检查点选择刚才训练的，推理引擎选 vllm 会更快，但是需要安装一些依赖
2.  点加载模型，然后对话即可

# 合并

    #model(基座模型)
    model_name_or_path: /root/autodl-tmp/DeepSeek-R1-Distill-Llama-8B
    #lora(自己训练的lora模型部分)
    adapter_name_or_path: /root/autodl-tmp/LLaMA-Factory/saves/DeepSeek-R1-8B-Distill/lora/train_2025-02-14-23-42-48
    # 模板(在 web 页面上看)
    template: deepseek3
    finetuning_type: lora

    #export
    export_dir: /root/autodl-tmp/DeepSeek-R1-Distill-Llama-8B-Instruct-merged
    # 每个文件大小
    export_size: 8
    export_device: cuda
    export_legacy_format: false

1.  创建 merge\_deepseek.yaml
2.  `llamafactory-cli export ./cust/merge_llama3_lora_stf.yaml`

合并之后也是 Safetensors 格式的

# LLAMA INDEX 加载数据检索

1.  下载 all-MiniLM-L12-v2 后面会用
    <https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2>

2.  新建下面的 python 文件。文件中的读取部分可以自行修改，不知道怎么改的话拿这段代码问 KIMI，效果还不错

<!---->

    import os
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from llama_index.core import Document,Settings
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.llms.huggingface import HuggingFaceLLM

    def recursive_directory_reader(directory_path):
        """递归读取目录下的所有 TS 文件，并返回 Document 列表"""
        documents = []
        
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                if file.endswith(".ts"):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        doc = Document(text=content, extra_info={"file_path": file_path})
                        documents.append(doc)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {e}")
        
        return documents

    # 构建索引
    embed_model = HuggingFaceEmbedding(
        model_name="D:/AI/all-MiniLM-L12-v2"
    )
    Settings.embed_model = embed_model

    llm = HuggingFaceLLM(
        model_name="D:\AI\DeepSeek-R1-Distill-Llama-8B",
        tokenizer_name="D:\AI\DeepSeek-R1-Distill-Llama-8B",
        model_kwargs={"trust_remote_code":True},
        tokenizer_kwargs={"trust_remote_code":True}
    )
    #设置全局的llm属性，这样在索引查询时会使用这个模型。
    Settings.llm = llm

    # 读取文档
    documents = recursive_directory_reader("D:/AI/data")

    # 构建索引
    index = VectorStoreIndex.from_documents(documents)

    # # 创建查询引擎
    query_engine = index.as_query_engine()

    # 保存索引
    index.storage_context.persist(persist_dir="save")
    print("索引已保存到 'save' 目录")

    # 加载模型和分词器
    model = AutoModelForCausalLM.from_pretrained("D:\AI\DeepSeek-R1-Distill-Llama-8B")
    tokenizer = AutoTokenizer.from_pretrained("D:\AI\DeepSeek-R1-Distill-Llama-8B")

    # 保存模型和分词器
    model.save_pretrained("D:\AI\exported_model")
    tokenizer.save_pretrained("D:\AI\exported_model")
    print("模型和分词器已导出到 'exported_model' 目录")

3\.  exported\_model 根目录创建 `Modelfile`

<!---->

    FROM .

4\.  下载 ollama
5\.  ollama create my-model

6.  ollama run my-model
7.  使用 <https://web.chatboxai.app/> 直接访问 ollama 调用试试

好了，基本的调试大模型就结束了，上面的工具还有更详细的参数，自行搜索即可
